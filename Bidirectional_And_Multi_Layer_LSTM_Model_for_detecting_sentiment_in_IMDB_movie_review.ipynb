{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BoDEkOLJ5Pa"
      },
      "source": [
        "# Sentiment Analysis of IMDB Dataset\n",
        "\n",
        "In the following, we'll be building a machine learning model to detect sentiment (i.e. detect if a sentence is positive or negative). This will be done on movie reviews, using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "\n",
        "We will use:\n",
        "- bidirectional LSTM\n",
        "- multi-layer LSTM (Deep LSTM)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEMM7MhKJ5Pi"
      },
      "source": [
        "## Preparing Data\n",
        "\n",
        "One of the main concepts of TorchText is the `Field`. These define how your data should be processed. In our sentiment classification task the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
        "\n",
        "The parameters of a `Field` specify how the data should be processed. \n",
        "\n",
        "We use the `TEXT` field to define how the review should be processed, and the `LABEL` field to process the sentiment. \n",
        "\n",
        "Our `TEXT` field has `tokenize='spacy'` as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io) tokenizer. If no `tokenize` argument is passed, the default is simply splitting the string on spaces.\n",
        "\n",
        "`LABEL` is defined by a `LabelField`, a special subset of the `Field` class specifically used for handling labels. We will explain the `dtype` argument later.\n",
        "\n",
        "\n",
        "We also set the random seeds for reproducibility. \n",
        "\n",
        "We'll be using *packed padded sequences*, which will make our LSTM only process the non-padded elements of our sequence, and for any padded element the `output` will be a zero tensor. To use packed padded sequences, we have to tell the RNN how long the actual sequences are. We do this by setting `include_lengths = True` for our `TEXT` field. This will cause `batch.text` to now be a tuple with the first element being our sentence (a numericalized tensor that has been padded) and the second element being the actual lengths of our sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTc0W6m7J5Pq"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "SEED = 2019\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUZtb_K3J5P0"
      },
      "source": [
        "Another handy feature of TorchText is that it has support for common datasets used in natural language processing (NLP). \n",
        "\n",
        "The following code automatically downloads the IMDb dataset and splits it into the canonical train/test splits as `torchtext.datasets` objects. It process the data using the `Fields` we have previously defined. The IMDb dataset consists of 50,000 movie reviews, each marked as being a positive or negative review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqllXgB4J5P2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7bdd170-6994-4fcd-87f5-bc2b2d52ee5d"
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:06<00:00, 12.6MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhgFdFV2cDLe"
      },
      "source": [
        "We can also check an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFHXf-6OcGW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b78fd6c-9dd0-4ff6-9271-b5f641483a33"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['Originally', 'I', 'rented', 'this', 'film', 'for', 'my', 'daughter', 'since', 'she', 'is', 'keen', 'on', 'soccer', '-', 'and', 'I', 'was', 'not', 'disappointed', 'that', 'way', '(', 'except', 'the', 'plot', 'interfered', 'with', 'the', 'soccer', 'scenes', ')', '.', 'As', 'a', 'dad', 'I', 'suppose', 'I', 'was', 'a', 'little', 'surprised', 'at', 'the', 'introduction', 'of', 'the', 'topic', 'of', 'lesbianism', '-', 'but', 'I', 'have', 'to', 'admit', 'that', 'as', 'a', 'parent', 'these', 'issues', 'are', 'completely', 'available', 'to', 'children', 'nowadays', '(', 'as', 'uncomfortable', 'as', 'I', 'feel', 'with', 'the', 'topic', ')', '.', 'In', 'a', 'way', 'this', 'emotion', 'was', 'a', 'segue', 'right', 'into', 'the', 'main', 'premise', 'of', 'the', 'film', '-', 'that', 'at', 'some', 'age', 'you', 'must', 'trust', 'your', 'children', 'to', 'make', 'their', 'own', 'choices', '.', 'This', 'dilemma', 'is', 'introduced', 'by', 'a', 'young', 'British', '-', 'born', 'teen', 'girl', '-', 'Jasminder', '(', 'Jess', ')', '-', 'of', 'east', '-', 'Indian', 'heritage', 'who', 'dreams', 'of', 'playing', 'professional', 'soccer', '.', 'The', 'pending', 'marriage', 'of', 'her', 'older', 'sister', 'in', 'a', 'traditional', 'Hindu', 'marriage', 'provides', 'many', 'rich', 'opportunities', 'for', 'her', 'to', 'explore', '(', 'in', 'a', 'nice', 'way', ')', 'her', 'hopes', 'and', 'fears', 'for', 'her', 'future', '.', '<', 'br', '/><br', '/>The', 'multi', '-', 'cultural', 'challenge', 'was', 'a', 'very', 'interesting', 'technique', 'to', 'explore', 'Jess', \"'s\", 'frustration', 'with', 'her', 'parents', 'expectations', 'for', 'her', '-', 'again', 'no', 'different', 'in', 'substance', 'than', 'most', 'child', '-', 'parent', 'relationships', '.', '<', 'br', '/><br', '/>In', 'SUMMARY', ',', 'the', 'soccer', 'scenes', 'are', 'GREAT', '(', 'lots', 'to', 'learn', 'in', 'slowmo', ')', 'and', 'while', 'I', 'did', \"n't\", 'need', 'the', 'storyline', '-', 'something', 'was', 'needed', 'to', 'keep', 'the', 'movie', 'going', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr5MiAWWJ5P8"
      },
      "source": [
        "The IMDb dataset only has train/test splits, so we need to create a validation set. We can do this with the `.split()` method. \n",
        "\n",
        "By default this splits 70/30, however by passing a `split_ratio` argument, we can change the ratio of the split, i.e. a `split_ratio` of 0.8 would mean 80% of the examples make up the training set and 20% make up the validation set. \n",
        "\n",
        "We also pass our random seed to the `random_state` argument, ensuring that we get the same train/validation split each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjAKSSEbJ5P9"
      },
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9MPhBu7ciym"
      },
      "source": [
        "We can see how many examples are in each split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGnAaOYscjiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df94419d-8f6f-4bde-e5c8-103115bb9b60"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqClKYLDdS6j"
      },
      "source": [
        "Next, we have to build a _vocabulary_. This is a effectively a look up table where every unique word in your data set has a corresponding _index_ (an integer).\n",
        "\n",
        "We do this as our machine learning model cannot operate on strings, only numbers. Each _index_ is used to construct a _one-hot_ vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and dimensionality is the total number of unique words in your vocabulary, commonly denoted by $V$.\n",
        "\n",
        "![](http://people.scs.carleton.ca/~majidkomeili/Teaching/COMP5900-F19/Images/One-hot.png)\n",
        "\n",
        "The number of unique words in our training set is over 100,000, which means that our one-hot vectors will have over 100,000 dimensions! This will make training slow and possibly won't fit onto the GPU. We only keep the top 25,000 most common words. What do we do with words that appear in examples but we have cut from the vocabulary? We replace them with a special _unknown_ or `<unk>` token. For example, if the sentence was \"This film is great and I love it\" but the word \"love\" was not in the vocabulary, it would become \"This film is great and I `<unk>` it\". The following builds the vocabulary, only keeping the most common `max_size` tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-MIc1MyJ5P_"
      },
      "source": [
        "Instead of having our word embeddings initialized randomly, they are initialized with these pre-trained pre-trained word embeddings.\n",
        "We get these vectors simply by specifying which vectors we want and passing it as an argument to `build_vocab`. `TorchText` handles downloading the vectors and associating them with the correct words in our vocabulary.\n",
        "\n",
        "Here, we'll be using the `\"glove.6B.100d\" vectors\"`. `glove` is the algorithm used to calculate the vectors, go [here](https://nlp.stanford.edu/projects/glove/) for more. `6B` indicates these vectors were trained on 6 billion tokens and `100d` indicates these vectors are 100-dimensional.\n",
        "\n",
        "The theory is that these pre-trained vectors already have words with similar semantic meaning close together in vector space, e.g. \"terrible\", \"awful\", \"dreadful\" are nearby. This gives our embedding layer a good initialization as it does not have to learn these relations from scratch.\n",
        "\n",
        "**Note**: these vectors are about 862MB, so watch out if you have a limited internet connection.\n",
        "\n",
        "By default, TorchText will initialize words in your vocabulary but not in your pre-trained embeddings to zero. We don't want this, and instead initialize them randomly by setting `unk_init` to `torch.Tensor.normal_`. This will now initialize those words via a Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOFTAByrJ5QB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ba423b-c0f6-409e-943f-1ab7089bfe96"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:42, 5.31MB/s]                           \n",
            " 99%|█████████▉| 397723/400000 [00:16<00:00, 24994.58it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSa_etgNiLR8"
      },
      "source": [
        "We can see the vocabulary directly using `itos` (**i**nt **to**  **s**tring) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4UWEdNfiWup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05135375-1b59-486d-e04b-104b749c1b90"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_SLP9F6rlmT"
      },
      "source": [
        "We can see the vocab size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTYMIHbYrnRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ae4016-094c-4564-a5bb-852288e089ea"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLSUiJrYf9ql"
      },
      "source": [
        "Why is the vocab size 25002 and not 25000? One of the addition tokens is the `<unk>` token and the other is a `<pad>` token.\n",
        "\n",
        "When we feed sentences into our model, we feed a _batch_ of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size. Thus, to ensure each sentence in the batch is the same size, any shorter than the longest within the batch are padded.\n",
        "\n",
        "![](http://people.scs.carleton.ca/~majidkomeili/Teaching/COMP5900-F19/Images/Pad.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVTcIWglJ5QD"
      },
      "source": [
        "The final step of preparing the data is creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
        "\n",
        "We'll use a `BucketIterator` which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
        "\n",
        "We also want to place the tensors returned by the iterator on the GPU (if one is available). PyTorch handles this using `torch.device`, we then pass this device to the iterator.\n",
        "\n",
        "Another thing for packed padded sequences all of the tensors within a batch need to be sorted by their lengths. This is handled in the iterator by setting `sort_within_batch = True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jo_0J88J5QE"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1J8kqmHJ5QG"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "The next stage is building the model that we'll eventually train and evaluate. Our three layers are an _embedding_ layer, our RNN, and a _linear_ layer. \n",
        "\n",
        "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the LSTM, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. We will initialize the embedding layer with GloVe embeddings.\n",
        "\n",
        "![](http://people.scs.carleton.ca/~majidkomeili/Teaching/COMP5900-F19/Images/Embedding.png)\n",
        "\n",
        "\n",
        "The MyRNN layer is our LSTM which takes in our dense vector $x_{t}$ and the previous hidden state $h_{t-1}$, and the previous cell memory $c_{t-1}$ which it uses to calculate the next hidden state, $h_t$.\n",
        "\n",
        "$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n",
        "\n",
        "Thus, the model using an LSTM looks something like (with the embedding layers omitted):\n",
        "\n",
        "![](http://people.scs.carleton.ca/~majidkomeili/Teaching/COMP5900-F19/Images/LSTM.png)\n",
        "\n",
        "The initial cell state, $c_0$, like the initial hidden state is initialized to a tensor of all zeros. \n",
        "\n",
        "Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension.\n",
        "\n",
        "\n",
        "### Bidirectional RNN\n",
        "\n",
        "As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the **last to the first** (a backward RNN). \n",
        "\n",
        "In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor.\n",
        "\n",
        "We make our sentiment prediction using a concatenation of the last hidden state from the forward LSTM (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward LSTM (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=f(h_T^\\rightarrow, h_T^\\leftarrow)$   \n",
        "\n",
        "\n",
        "### Multi-layer RNN\n",
        "\n",
        "In multi-layer LSTM (also called *deep LSTM*) we add additional LSTMs on top of the initial standard LSTM, where each LSTM added is another *layer*. The hidden state output by the first (bottom) LSTM at time-step $t$ will be the input to the LSTM above it at time step $t$. The prediction is then made from the final hidden state of the final (highest) layer.\n",
        "\n",
        "\n",
        "### Implementation Details\n",
        "\n",
        "We are not going to learn the embedding for the `<pad>` token. This is because we want to explitictly tell our model that padding tokens are irrelevant to determining the sentiment of a sentence. This means the embedding for the pad token will remain at what it is initialized to (we initialize it to all zeros later). We do this by passing the index of our pad token as the `padding_idx` argument to the `nn.Embedding` layer.\n",
        "\n",
        "To use an RNN instead of the LSTM, you can use `nn.RNN` instead of `nn.LSTM`. Also, note that the LSTM returns the `output` and a tuple of the final `hidden` state and the final `cell` state, whereas the standard RNN only returned the `output` and final `hidden` state. \n",
        "\n",
        "As the final hidden state of our LSTM has both a forward and a backward component, which will be concatenated together, the size of the input to the `nn.Linear` layer is twice that of the hidden dimension size.\n",
        "\n",
        "Implementing bidirectionality and adding additional layers are done by passing values for the `num_layers` and `bidirectional` arguments for the RNN/LSTM. \n",
        "\n",
        "The LSTM has a `dropout` argument which adds dropout on the connections between hidden states in one layer to hidden states in the next layer.\n",
        "\n",
        "As we are passing the lengths of our sentences to be able to use packed padded sequences, we have to add a second argument, `text_lengths`, to `forward`. \n",
        "\n",
        "Before we pass our embeddings to the LSTM, we need to pack them, which we do with `nn.utils.rnn.packed_padded_sequence`. This will cause our LSTM to only process the non-padded elements of our sequence. The LSTM will then return `packed_output` (a packed sequence) as well as the `hidden` and `cell` states (both of which are tensors). Without packed padded sequences, `hidden` and `cell` are tensors from the last element in the sequence, which will most probably be a pad token, however when using packed padded sequences they are both from the last non-padded element in the sequence.\n",
        "\n",
        "We then unpack the output sequence, with `nn.utils.rnn.pad_packed_sequence`, to transform it from a packed sequence to a tensor. The elements of `output` from padding tokens will be zero tensors (tensors where every element is zero). Usually, we only have to unpack output if we are going to use it later on in the model. Although we aren't in this case, we still unpack the sequence just to show how it is done.\n",
        "\n",
        "The final hidden state, `hidden`, has a shape of _[num layers $\\times$ num directions, batch size, hid dim]_. These are ordered: _[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1]_. As we want the final (top) layer forward and backward hidden states, we get `hidden[2,:,:]` and `hidden[3,:,:]`, and concatenate them together before passing them to the linear layer (after applying dropout). \n",
        "\n",
        "The input dimension is the dimension of the one-hot vectors, which is equal to the vocabulary size i.e. 25,002. \n",
        "\n",
        "The embedding dimension is the size of the dense word vectors i.e. 100.\n",
        "\n",
        "The hidden dimension is the size of the hidden states i.e. 200.\n",
        "\n",
        "The output dimension is usually the number of classes, however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, i.e. a single scalar real number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ4oQYUXe0gu"
      },
      "source": [
        "## Question1.1 Model with Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7KK0M9VbG4F"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MyRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        vocab_size = 25002\n",
        "        embedding_dim = 100\n",
        "        hidden_dim = 200\n",
        "        output_dim = 1\n",
        "        n_layers = 2\n",
        "        bidirectional = True\n",
        "        dropout = 0.5\n",
        "        pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu()) #text_lengths.cpu()\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)        \n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * 2]\n",
        "        #output over padding tokens are zero tensors\n",
        "        \n",
        "        #hidden = [num layers * 2, batch size, hid dim]\n",
        "        #cell = [num layers * 2, batch size, hid dim]\n",
        "        \n",
        "        #concat the final forward (hidden[2,:,:]) and backward (hidden[3,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[2,:,:], hidden[3,:,:]), dim = 1))\n",
        "                        \n",
        "        #hidden = [batch size, hid dim * 2]\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itQt1PwoJ5QP"
      },
      "source": [
        "We now create an instance of our RNN class.\n",
        "We'll print out the total number of parameters in our model. We also print the details of the number of parameters in each layer. For example, ()'lstm.weight_ih_l0', 80000) indicates that there are 100$\\times$200 parameters that connect the embeddings (dim=100) to the hidden layer (dim=200), and there are 3X additional parameters coressponding to the three gates in LSTM. Hence a total of 100$\\times$200$\\times$4=80,000 parameters. Likewise ('lstm.weight_hh_l0', 160000) indicates that there are 200$\\times$200 parameters that connect the previous hidden state (dim=200) to the hidden layer (dim=200), and there are 3X additional parameters coressponding to the three gates in LSTM. Hence a total of 200$\\times$200$\\times$4=160,000 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtS6c2nIJ5QQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453aa9e1-0023-4807-a50f-aa67798788dc"
      },
      "source": [
        "model = MyRNN()\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "print(MyRNN())\n",
        "[(n, p.numel()) for n, p in MyRNN().named_parameters()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 3,947,001 trainable parameters\n",
            "MyRNN(\n",
            "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
            "  (lstm): LSTM(100, 200, num_layers=2, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=400, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('embedding.weight', 2500200),\n",
              " ('lstm.weight_ih_l0', 80000),\n",
              " ('lstm.weight_hh_l0', 160000),\n",
              " ('lstm.bias_ih_l0', 800),\n",
              " ('lstm.bias_hh_l0', 800),\n",
              " ('lstm.weight_ih_l0_reverse', 80000),\n",
              " ('lstm.weight_hh_l0_reverse', 160000),\n",
              " ('lstm.bias_ih_l0_reverse', 800),\n",
              " ('lstm.bias_hh_l0_reverse', 800),\n",
              " ('lstm.weight_ih_l1', 320000),\n",
              " ('lstm.weight_hh_l1', 160000),\n",
              " ('lstm.bias_ih_l1', 800),\n",
              " ('lstm.bias_hh_l1', 800),\n",
              " ('lstm.weight_ih_l1_reverse', 320000),\n",
              " ('lstm.weight_hh_l1_reverse', 160000),\n",
              " ('lstm.bias_ih_l1_reverse', 800),\n",
              " ('lstm.bias_hh_l1_reverse', 800),\n",
              " ('fc.weight', 400),\n",
              " ('fc.bias', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNCE9FTAJ5QU"
      },
      "source": [
        "Next we will copy the pre-trained word embeddings we loaded earlier into the `embedding` layer of our model.\n",
        "\n",
        "We retrieve the embeddings from the field's vocab, and check they're the correct size, _[vocab size, embedding dim]_ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umTGqCRxJ5QV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3142de5e-c2e7-451a-bf8c-9801c9c14eb6"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKeNhh2rJ5Qb"
      },
      "source": [
        "We then replace the initial weights of the `embedding` layer with the pre-trained embeddings.\n",
        "\n",
        "**Note**: this should always be done on the `weight.data` and not the `weight`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWj26h15J5Qc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df376178-75ca-4fc5-ec59-bfe56cb76e5c"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6946, -1.3275, -1.1976,  ...,  0.6238, -0.8446,  0.0270],\n",
              "        [-1.1092, -0.0392, -0.5137,  ..., -0.0642,  0.9299,  0.3551],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.0389,  0.1470, -0.0629,  ..., -0.6497, -0.5434,  0.8196],\n",
              "        [-0.1910, -0.3052, -0.3037,  ..., -0.0199, -0.2853,  1.1701],\n",
              "        [ 0.1268,  0.3074,  0.2969,  ..., -0.3262, -0.5281,  0.3048]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8TkoGLmJ5Qf"
      },
      "source": [
        "As our `<unk>` and `<pad>` token aren't in the pre-trained vocabulary they have been initialized using `unk_init` (an $\\mathcal{N}(0,1)$ distribution) when building our vocab. It is preferable to initialize them both to all zeros to explicitly tell our model that, initially, they are irrelevant for determining sentiment. \n",
        "\n",
        "We do this by manually setting their row in the embedding weights matrix to zeros. We get their row by finding the index of the tokens, which we have already done for the padding index.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ibhDxYAJ5Qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bbfd09-cdd0-44f2-ed6b-d703b5b190de"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.0389,  0.1470, -0.0629,  ..., -0.6497, -0.5434,  0.8196],\n",
            "        [-0.1910, -0.3052, -0.3037,  ..., -0.0199, -0.2853,  1.1701],\n",
            "        [ 0.1268,  0.3074,  0.2969,  ..., -0.3262, -0.5281,  0.3048]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNGd-W25J5Qi"
      },
      "source": [
        "We can now see the first two rows of the embedding weights matrix have been set to zeros. As we passed the index of the pad token to the `padding_idx` of the embedding layer it will remain zeros throughout training, however the `<unk>` token embedding will be learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc1K-xyJJ5Qk"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE63pcQyJ5Ql"
      },
      "source": [
        "Now we'll set up the training and then train the model. First, we'll create an optimizer. This is the algorithm we use to update the parameters of the module. We will use `Adam` instead of `SGD` that we used in Assignment 1. SGD updates all parameters with the same learning rate and choosing this learning rate can be tricky. `Adam` adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eaY-8OMJ5Qo"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31M4H5clJ5Qt"
      },
      "source": [
        "Next, we'll define our loss function. In PyTorch this is commonly called a criterion. \n",
        "\n",
        "The loss function here is _binary cross entropy with logits_. Our model currently outputs an unbound real number. As our labels are either 0 or 1, we want to restrict the predictions to a number between 0 and 1. We do this using the _sigmoid_. We then use this bound scalar to calculate the loss using binary cross entropy. \n",
        "\n",
        "The `BCEWithLogitsLoss` criterion carries out both the sigmoid and the binary cross entropy steps.\n",
        "\n",
        "Using `.to`, we can place the model and the criterion on the GPU (if we have one). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HJoSnPYJ5Qu"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itLdtqoYJ5Qy"
      },
      "source": [
        "Our criterion function calculates the loss, however we have to write our function to calculate the accuracy. \n",
        "\n",
        "This function first feeds the predictions through a sigmoid layer, squashing the values between 0 and 1, we then round them to the nearest integer. This rounds any value greater than 0.5 to 1 (a positive sentiment) and the rest to 0 (a negative sentiment).\n",
        "\n",
        "We then calculate how many rounded predictions equal the actual labels and average it across the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv7D8WTEJ5Qy"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JAYibx-J5Q0"
      },
      "source": [
        "The `train` function iterates over all examples, one batch at a time. \n",
        "\n",
        "`model.train()` is used to put the model in \"training mode\", which turns on _dropout_ and _batch normalization_. Though we aren't using _batch normalization_ in this model.\n",
        "\n",
        "For each batch, we first zero the gradients. Each parameter in a model has a `grad` attribute which stores the gradient calculated by the `criterion`. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
        "\n",
        "As we have set `include_lengths = True`, our `batch.text` is a tuple with the first element being the numericalized tensor and the second element being the actual lengths of each sequence. We separate these into their own variables, `text` and `text_lengths`, before passing them to the model.\n",
        "\n",
        "We then feed the batch of sentences, `batch.text`, into the model. Note, you do not need to do `model.forward(batch.text)`, simply calling the model works. The `squeeze` is needed as the predictions are initially size _[batch size, 1]_, and we need to remove the dimension of size 1 as PyTorch expects the predictions input to our criterion function to be of size _[batch size]_.\n",
        "\n",
        "The loss and accuracy are then calculated using our predictions and the labels, `batch.label`, with the loss being averaged over all examples in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW0zocRTJ5Q1"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.text\n",
        "        \n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUoWKkQSJ5Q-"
      },
      "source": [
        "`evaluate` is similar to `train`, with a few modifications as you don't want to update the parameters when evaluating.\n",
        "\n",
        "`model.eval()` puts the model in \"evaluation mode\", this turns off _dropout_ and _batch normalization_. Though, we are not using _batch normalization_ in this model.\n",
        "\n",
        "No gradients are calculated on PyTorch operations inside the `with no_grad()` block. This causes less memory to be used and speeds up computation.\n",
        "\n",
        "The rest of the function is the same as `train`, with the removal of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as we do not update the model's parameters when evaluating.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-7ioK_CJ5RC"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS8B7KHuJ5RJ"
      },
      "source": [
        "And also create a nice function to tell us how long our epochs are taking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5amMxgFXJ5RK"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQxby7JJ5RM"
      },
      "source": [
        "## Model Training for Q1.1: \n",
        "Finally, we train our model...\n",
        "At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB9cP3E-amwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d36498-1b5d-46ad-9d58-9f5b09ed73a6"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.618 | Train Acc: 64.78%\n",
            "\t Val. Loss: 0.535 |  Val. Acc: 75.60%\n",
            "Epoch: 02 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.500 | Train Acc: 76.02%\n",
            "\t Val. Loss: 0.408 |  Val. Acc: 82.02%\n",
            "Epoch: 03 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.356 | Train Acc: 84.96%\n",
            "\t Val. Loss: 0.346 |  Val. Acc: 85.45%\n",
            "Epoch: 04 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.271 | Train Acc: 89.25%\n",
            "\t Val. Loss: 0.359 |  Val. Acc: 85.05%\n",
            "Epoch: 05 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.197 | Train Acc: 92.54%\n",
            "\t Val. Loss: 0.330 |  Val. Acc: 87.45%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J25W6ncKJ5RP"
      },
      "source": [
        "Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxsOGORMJ5RR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f41a872-1228-488f-a0f6-4da9814f15f4"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.367 | Test Acc: 86.12%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgdXg4OYKBEz"
      },
      "source": [
        "## Question 1.2: Model with 2 bi-RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlnJ2kRoKomc"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MyRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        vocab_size = 25002\n",
        "        embedding_dim = 100\n",
        "        hidden_dim = 200\n",
        "        output_dim = 1\n",
        "        n_layers = 2\n",
        "        bidirectional = True\n",
        "        dropout = 0.5\n",
        "        pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu()) #text_lengths.cpu()\n",
        "        \n",
        "        packed_output, (hidden) = self.rnn(packed_embedded)        \n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * 2]\n",
        "        #output over padding tokens are zero tensors\n",
        "        \n",
        "        #hidden = [num layers * 2, batch size, hid dim]\n",
        "        #cell = [num layers * 2, batch size, hid dim]\n",
        "        \n",
        "        #concat the final forward (hidden[2,:,:]) and backward (hidden[3,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[2,:,:], hidden[3,:,:]), dim = 1))\n",
        "                        \n",
        "        #hidden = [batch size, hid dim * 2]\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j13F1JCNMupp",
        "outputId": "c71ee003-a7d5-4879-b892-766070a0743d"
      },
      "source": [
        "model = MyRNN()\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "print(MyRNN())\n",
        "[(n, p.numel()) for n, p in MyRNN().named_parameters()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,862,201 trainable parameters\n",
            "MyRNN(\n",
            "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
            "  (rnn): RNN(100, 200, num_layers=2, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=400, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('embedding.weight', 2500200),\n",
              " ('rnn.weight_ih_l0', 20000),\n",
              " ('rnn.weight_hh_l0', 40000),\n",
              " ('rnn.bias_ih_l0', 200),\n",
              " ('rnn.bias_hh_l0', 200),\n",
              " ('rnn.weight_ih_l0_reverse', 20000),\n",
              " ('rnn.weight_hh_l0_reverse', 40000),\n",
              " ('rnn.bias_ih_l0_reverse', 200),\n",
              " ('rnn.bias_hh_l0_reverse', 200),\n",
              " ('rnn.weight_ih_l1', 80000),\n",
              " ('rnn.weight_hh_l1', 40000),\n",
              " ('rnn.bias_ih_l1', 200),\n",
              " ('rnn.bias_hh_l1', 200),\n",
              " ('rnn.weight_ih_l1_reverse', 80000),\n",
              " ('rnn.weight_hh_l1_reverse', 40000),\n",
              " ('rnn.bias_ih_l1_reverse', 200),\n",
              " ('rnn.bias_hh_l1_reverse', 200),\n",
              " ('fc.weight', 400),\n",
              " ('fc.bias', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO1PTAxONXL7",
        "outputId": "aab12397-47ff-4885-fe24-b4f3d03a6bd3"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.681 | Train Acc: 57.11%\n",
            "\t Val. Loss: 0.690 |  Val. Acc: 57.25%\n",
            "Epoch: 02 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.649 | Train Acc: 62.81%\n",
            "\t Val. Loss: 0.659 |  Val. Acc: 60.69%\n",
            "Epoch: 03 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.639 | Train Acc: 63.18%\n",
            "\t Val. Loss: 0.643 |  Val. Acc: 63.16%\n",
            "Epoch: 04 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.608 | Train Acc: 66.93%\n",
            "\t Val. Loss: 0.636 |  Val. Acc: 63.80%\n",
            "Epoch: 05 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.555 | Train Acc: 71.10%\n",
            "\t Val. Loss: 0.575 |  Val. Acc: 71.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bew0Y4N3TQJK",
        "outputId": "86681985-59e5-4f50-cc51-d2c7d4d1db74"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.596 | Test Acc: 69.51%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d52RPsCZiwr"
      },
      "source": [
        "## Question 1.3: RNN with 1 layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlHbkClIZw-L"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MyRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        vocab_size = 25002\n",
        "        embedding_dim = 100\n",
        "        hidden_dim = 200\n",
        "        output_dim = 1\n",
        "        n_layers = 1\n",
        "        bidirectional = True\n",
        "        dropout = 0.5\n",
        "        pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, \n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,  \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu()) #text_lengths.cpu()\n",
        "        \n",
        "        packed_output, (hidden) = self.rnn(packed_embedded)           \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "    \n",
        "        #and apply dropout self.dropout(torch.cat((hidden[2,:,:], hidden[3,:,:]), dim = 1))  \n",
        "        hidden= self.dropout(torch.cat((hidden[0,:,:], hidden[1,:,:]), dim = 1))   \n",
        "        #hidden = self.dropout(hidden[1,:,:])\n",
        "                        \n",
        "        #hidden = [batch size, hid dim * 2]\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYn_tzzDZ-A4",
        "outputId": "a3859c09-559d-4aa2-8a36-df625b2768c9"
      },
      "source": [
        "model = MyRNN()\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "print(MyRNN())\n",
        "[(n, p.numel()) for n, p in MyRNN().named_parameters()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,621,401 trainable parameters\n",
            "MyRNN(\n",
            "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
            "  (rnn): RNN(100, 200, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=400, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('embedding.weight', 2500200),\n",
              " ('rnn.weight_ih_l0', 20000),\n",
              " ('rnn.weight_hh_l0', 40000),\n",
              " ('rnn.bias_ih_l0', 200),\n",
              " ('rnn.bias_hh_l0', 200),\n",
              " ('rnn.weight_ih_l0_reverse', 20000),\n",
              " ('rnn.weight_hh_l0_reverse', 40000),\n",
              " ('rnn.bias_ih_l0_reverse', 200),\n",
              " ('rnn.bias_hh_l0_reverse', 200),\n",
              " ('fc.weight', 400),\n",
              " ('fc.bias', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVrDBFldTv71",
        "outputId": "33114fe0-c164-4b99-e0f9-c1e499153614"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.682 | Train Acc: 57.13%\n",
            "\t Val. Loss: 0.690 |  Val. Acc: 53.23%\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.690 | Train Acc: 55.25%\n",
            "\t Val. Loss: 0.662 |  Val. Acc: 59.54%\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.628 | Train Acc: 64.43%\n",
            "\t Val. Loss: 0.633 |  Val. Acc: 64.02%\n",
            "Epoch: 04 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.601 | Train Acc: 67.15%\n",
            "\t Val. Loss: 0.646 |  Val. Acc: 62.77%\n",
            "Epoch: 05 | Epoch Time: 0m 9s\n",
            "\tTrain Loss: 0.519 | Train Acc: 74.29%\n",
            "\t Val. Loss: 0.616 |  Val. Acc: 67.56%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrxDnSLhUHHD",
        "outputId": "c5ed4431-e6e9-49c6-c782-83f6a6cd7282"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.633 | Test Acc: 66.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SS6C19FiTKc"
      },
      "source": [
        "## Question 1.4: RNN without bidirectional parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv2gW9YKicbU"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MyRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        vocab_size = 25002\n",
        "        embedding_dim = 100\n",
        "        hidden_dim = 200\n",
        "        output_dim = 1\n",
        "        n_layers = 1\n",
        "        dropout = 0.5\n",
        "        pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, \n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,                            \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu()) #text_lengths.cpu()\n",
        "        \n",
        "        packed_output, (hidden) = self.rnn(packed_embedded)        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "   \n",
        "        hidden = self.dropout(hidden[0,:,:])\n",
        "                        \n",
        "        #hidden = [batch size, hid dim * 2]\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHplPwxVi3cQ",
        "outputId": "6ee7a9fb-b2d0-4fdc-8f2d-87de5bc7b594"
      },
      "source": [
        "model = MyRNN()\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "print(MyRNN())\n",
        "[(n, p.numel()) for n, p in MyRNN().named_parameters()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,560,801 trainable parameters\n",
            "MyRNN(\n",
            "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
            "  (rnn): RNN(100, 200, dropout=0.5)\n",
            "  (fc): Linear(in_features=200, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('embedding.weight', 2500200),\n",
              " ('rnn.weight_ih_l0', 20000),\n",
              " ('rnn.weight_hh_l0', 40000),\n",
              " ('rnn.bias_ih_l0', 200),\n",
              " ('rnn.bias_hh_l0', 200),\n",
              " ('fc.weight', 200),\n",
              " ('fc.bias', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOHL27ur0njB",
        "outputId": "c2f70006-20bd-47e3-d200-1645f196126f"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.690 | Train Acc: 54.73%\n",
            "\t Val. Loss: 0.686 |  Val. Acc: 54.76%\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.681 | Train Acc: 56.43%\n",
            "\t Val. Loss: 0.676 |  Val. Acc: 58.11%\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.659 | Train Acc: 60.83%\n",
            "\t Val. Loss: 0.631 |  Val. Acc: 66.28%\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.657 | Train Acc: 60.83%\n",
            "\t Val. Loss: 0.647 |  Val. Acc: 62.52%\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.614 | Train Acc: 66.64%\n",
            "\t Val. Loss: 0.666 |  Val. Acc: 58.81%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBh5NlLtTj2T",
        "outputId": "113e5db3-11f3-4f3e-c055-b0627eaec458"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.643 | Test Acc: 64.10%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2mXPGcCJ5RT"
      },
      "source": [
        "## User Input\n",
        "\n",
        "We can now use our model to predict the sentiment of any sentence we give it.\n",
        "\n",
        "Our `predict_sentiment` function does a few things:\n",
        "- sets the model to evaluation mode\n",
        "- tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n",
        "- indexes the tokens by converting them into their integer representation from our vocabulary\n",
        "- gets the length of our sequence\n",
        "- converts the indexes, which are a Python list into a PyTorch tensor\n",
        "- add a batch dimension by `unsqueeze`ing \n",
        "- converts the length into a tensor\n",
        "- squashes the output prediction from a real number between 0 and 1 with the `sigmoid` function\n",
        "- converts the tensor holding a single value into an integer with the `item()` method\n",
        "\n",
        "We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfRSxLBBJ5RU"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUMXexC3J5RV"
      },
      "source": [
        "An example negative review..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8DNTn1DJ5RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12253144-411c-41bc-9bea-408ef7c98b57"
      },
      "source": [
        "predict_sentiment(model, \"This film is terrible\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0005689388490281999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxmDk-g7J5RX"
      },
      "source": [
        "An example positive review..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqU0LCgMJ5RY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325adad5-2206-4bcc-9b81-6921a4f57ad4"
      },
      "source": [
        "predict_sentiment(model, \"This film is great\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9938940405845642"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh2buAvCuzxF",
        "outputId": "a24ca965-b201-4104-c1a8-3dc735f102d2"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.576 | Train Acc: 70.27%\n",
            "\t Val. Loss: 0.590 |  Val. Acc: 70.41%\n",
            "Epoch: 02 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.485 | Train Acc: 78.95%\n",
            "\t Val. Loss: 0.597 |  Val. Acc: 72.42%\n",
            "Epoch: 03 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.445 | Train Acc: 81.66%\n",
            "\t Val. Loss: 0.559 |  Val. Acc: 72.52%\n",
            "Epoch: 04 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.398 | Train Acc: 84.49%\n",
            "\t Val. Loss: 0.576 |  Val. Acc: 73.93%\n",
            "Epoch: 05 | Epoch Time: 0m 4s\n",
            "\tTrain Loss: 0.351 | Train Acc: 87.03%\n",
            "\t Val. Loss: 0.584 |  Val. Acc: 73.32%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}